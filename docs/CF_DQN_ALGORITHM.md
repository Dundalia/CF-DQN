# CF-DQN Algorithm: Training Procedure

## Overview

CF-DQN (Characteristic Function Deep Q-Network) is a **distributional reinforcement learning** algorithm that represents the distribution of returns in the **frequency domain** using characteristic functions (CFs). Unlike C51 which uses discrete probability masses over return bins, CF-DQN learns complex-valued functions Ï†(Ï‰) that encode distributional information through phase and magnitude.

**Key Insight**: The characteristic function Ï†(Ï‰) = E[exp(iÏ‰G)] is the Fourier transform of the return distribution. The mean return E[G] can be extracted via Ï†'(0) = iE[G].

---

## Algorithm Components

### 1. Network Architecture

**QNetwork**: Neural network outputting characteristic functions for each action.

**Structure**:
```
Input: State observation (4-dim for CartPole)
  â†“
Layer 1: Linear(obs_dim â†’ 120) + ReLU
  â†“
Layer 2: Linear(120 â†’ 84) + ReLU
  â†“
Layer 3: Linear(84 â†’ n_actions Ã— K Ã— 2)
  â†“
Reshape: [batch, n_actions, K, 2]
  â†“
Convert to Complex: Ï†(s,a,Ï‰) âˆˆ â„‚^K  (real + iÂ·imag)
```

**Output Interpretation**:
- `Ï†(s, a, Ï‰)`: Complex-valued CF at K frequency points for action a in state s
- Shape: `[batch_size, n_actions, K]` where K = 128 frequency points
- **No architectural constraints**: The network can output arbitrary complex values

**Two Networks**:
1. **q_network**: Policy network (updated every training step)
2. **target_network**: Target network (updated every 500 steps)

---

### 2. Frequency Grid Construction

**Frequency Domain**: Ï‰ âˆˆ [-W, W] where W = `freq_max` = 5.0

**Three-Density Grid Strategy** (`make_omega_grid`):

| Region | Range | Points Allocated | Purpose |
|--------|-------|------------------|---------|
| **Center** | \|Ï‰\| â‰¤ 0.1W | 50% (64 points) | Dense sampling near Ï‰=0 for accurate mean extraction |
| **Middle** | 0.1W < \|Ï‰\| â‰¤ 0.4W | 30% (38 points) | Medium density transition |
| **Tails** | 0.4W < \|Ï‰\| â‰¤ W | 20% (26 points) | Sparse coverage of high frequencies |

**Rationale**: 
- Mean extraction requires accurate Ï†'(0), so we need high resolution near Ï‰=0
- High frequencies (large Ï‰) contribute little to mean but capture tail behavior
- Grid spans [-5, 5] to match CartPole Q-value range (~10-500)

**Key Parameters**:
- `n_frequencies = 128`: Total grid points (K)
- `freq_max = 5.0`: Maximum frequency
- `collapse_max_w = 2.0`: Use frequencies in [-2, 2] for mean extraction

---

### 3. Training Loop

#### **Step 1: Data Collection**

Standard Îµ-greedy exploration:
```python
if random() < Îµ:
    action = random_action()
else:
    # Collapse CF to scalar Q-values for action selection
    Q(s, a) = collapse_cf_to_mean(Ï†(s, a, Ï‰))
    action = argmax_a Q(s, a)
```

Store transitions `(s, a, r, s', done)` in replay buffer.

#### **Step 2: Sample Mini-Batch**

Sample batch of size 128 from replay buffer:
```
batch = {observations, actions, rewards, next_observations, dones}
```

#### **Step 3: Compute Target CF**

This is the **core CF Bellman operator**:

**a) Select Next Action** (Double DQN style):
```python
# Use target network to get CFs for all actions in next state
Ï†_target(s', a', Î³Ï‰) for all a'  # [batch, n_actions, K]

# Collapse to scalar Q-values
Q_target(s', a') = collapse_cf_to_mean(Ï†_target(s', a', Ï‰))

# Select greedy action
a* = argmax_a' Q_target(s', a')
```

**b) Interpolate CF at Scaled Frequencies**:
```python
# Get CF for greedy action: Ï†_target(s', a*, Ï‰)
# Need to evaluate at Î³Ï‰ (discount scaling in frequency domain)
Ï†_future(s', a*, Î³Ï‰) = interpolate_cf_polar(
    target_omegas = Î³ Â· Ï‰,
    grid_omegas = Ï‰,
    cf = Ï†_target(s', a*, Ï‰)
)
```

**Interpolation Method** (`interpolate_cf_polar`):
1. Decompose into polar form: Ï† = |Ï†| Â· exp(iÂ·arg(Ï†))
2. Unwrap phase to handle 2Ï€ discontinuities
3. Linearly interpolate magnitude and unwrapped phase separately
4. Reconstruct: Ï†_interp = |Ï†|_interp Â· exp(iÂ·arg(Ï†)_interp)

**c) Compute Reward CF**:
```python
# Characteristic function of immediate reward: exp(iÏ‰r)
Ï†_reward(Ï‰) = exp(i Â· Ï‰ Â· r)  # [batch, K]
```

For CartPole with r=1.0: Ï†_reward(Ï‰) = exp(iÏ‰) = cos(Ï‰) + iÂ·sin(Ï‰)

**d) Apply CF Bellman Operator**:
```python
# Terminal states: future return is 0, so Ï†_future = 1 (CF of zero)
Ï†_future = (1 - done) Â· Ï†_future(s', a*, Î³Ï‰) + done Â· 1

# CF composition: Ï†(r + Î³G') = Ï†_r Â· Ï†_G' (independence assumption)
Ï†_target = Ï†_reward(Ï‰) Â· Ï†_future  # [batch, K]
```

**Mathematical Justification**:
- If r and G' are independent: Ï†_{r+Î³G'}(Ï‰) = E[exp(iÏ‰(r + Î³G'))] = E[exp(iÏ‰r)] Â· E[exp(iÏ‰Î³G')]
- In practice, they're conditionally independent given (s, a)
- Frequency scaling: Ï†_{Î³G'}(Ï‰) = Ï†_G'(Î³Ï‰)

#### **Step 4: Compute Prediction CF**

```python
# Get CF for the action that was actually taken
Ï†_pred(s, a, Ï‰) = q_network.get_action(s, a)  # [batch, K]
```

#### **Step 5: Compute Loss**

**a) CF Matching Loss** (L2 in frequency domain):
```python
L_cf = mean(|Ï†_pred(Ï‰) - Ï†_target(Ï‰)|Â²)
     = mean((Re[Ï†_pred - Ï†_target])Â² + (Im[Ï†_pred - Ï†_target])Â²)
```

**b) Normalization Constraint Penalty**:

Valid characteristic functions must satisfy **Ï†(0) = 1** (zeroth moment).

```python
# Soft constraint: penalize deviation from |Ï†(0)| = 1
idx_zero = argmin|Ï‰|  # Index of Ï‰ closest to 0

L_penalty = mean((|Ï†_pred(0)| - 1)Â² + (|Ï†_target(0)| - 1)Â²)
```

**Why Soft Penalty Instead of Hard Normalization?**
- Hard normalization `Ï† â† Ï†/Ï†(0)` caused mode collapse (Ï†(Ï‰) â‰ˆ 1 for all Ï‰)
- Dividing both predictions and targets removed gradient information
- Soft penalty preserves phase derivatives (crucial for mean extraction)
- Applied to both predictions AND targets (targets also violate constraint after interpolation)

**c) Total Loss**:
```python
L_total = L_cf + penalty_weight Â· L_penalty

where penalty_weight = 5.0
```

#### **Step 6: Gradient Update**

```python
optimizer.zero_grad()
L_total.backward()
optimizer.step()
```

**Optimizer**: Adam with lr=2.5e-4, eps=0.01/batch_size

#### **Step 7: Target Network Update**

Every 500 steps:
```python
target_network.load_state_dict(q_network.state_dict())
```

---

## Mean Extraction: CF â†’ Scalar Q-values

**Method**: Gaussian Collapse (`collapse_cf_to_mean`)

**Theory**: For locally Gaussian CF near Ï‰=0:
```
log Ï†(Ï‰) â‰ˆ iÎ¼Ï‰ - 0.5ÏƒÂ²Ï‰Â²

âŸ¹ phase(Ï†(Ï‰)) â‰ˆ Î¼Ï‰

âŸ¹ Ï†'(0) = iÎ¼  âŸ¹  Î¼ = Im[Ï†'(0)]
```

**Implementation** (phase slope fitting):
1. **Select low frequencies**: Use only Ï‰ âˆˆ [-collapse_max_w, collapse_max_w] = [-2, 2]
2. **Compute unwrapped phase**: Î¸(Ï‰) = arg(Ï†(Ï‰)) with discontinuities removed
3. **Linear regression**: Fit Î¸(Ï‰) â‰ˆ Î¼Ï‰ (line through origin)
   ```
   Î¼ = sum(Ï‰ Â· Î¸(Ï‰)) / sum(Ï‰Â²)
   ```
4. **Return mean**: Q(s, a) = Î¼

**Why This Works**:
- Near Ï‰=0, the CF is smooth and phase is approximately linear
- Slope of phase gives the mean return
- Robust to noise if |Ï†(0)| â‰ˆ 1

**Alternative Method** (finite difference - diagnostic only):
```python
Ï†'(0) â‰ˆ (Ï†(Ï‰â‚) - Ï†(-Ï‰â‚)) / (2Ï‰â‚)
Q(s, a) = Im[Ï†'(0)]
```
Used for validation but not in main algorithm.

---

## Hyperparameter Summary

### Core Parameters
| Parameter | Value | Description |
|-----------|-------|-------------|
| `n_frequencies` | 128 | Number of frequency points K |
| `freq_max` | 5.0 | Maximum frequency (grid: [-5, 5]) |
| `collapse_max_w` | 2.0 | Frequency range for mean extraction |
| `penalty_weight` | 5.0 | Weight for Ï†(0)=1 constraint |

### Standard DQN Parameters
| Parameter | Value | Description |
|-----------|-------|-------------|
| `learning_rate` | 2.5e-4 | Adam optimizer learning rate |
| `batch_size` | 128 | Mini-batch size |
| `buffer_size` | 10,000 | Replay buffer capacity |
| `gamma` | 0.99 | Discount factor |
| `target_network_frequency` | 500 | Steps between target updates |
| `learning_starts` | 10,000 | Steps before training begins |
| `train_frequency` | 10 | Train every 10 steps |
| `start_e` / `end_e` | 1.0 / 0.05 | Îµ-greedy exploration |
| `exploration_fraction` | 0.5 | Fraction of training for exploration |

---

## Diagnostic Logging

### Training Metrics
- `losses/loss`: Total loss (CF MSE + penalty)
- `losses/phi_zero_penalty`: Constraint violation term
- `losses/q_values`: Mean Q-value from Gaussian collapse
- `losses/q_values_all_mean/max`: Q-value statistics across actions

### CF Quality Indicators
- `cf/magnitude_at_zero`: |Ï†(0)| (should be â‰ˆ 1.0)
- `cf/target_magnitude_at_zero`: |Ï†_target(0)| (should be â‰ˆ 1.0)
- `cf/max_magnitude`: max|Ï†(Ï‰)| (should be â‰¤ 1.0, valid CF constraint)
- `cf/mean_magnitude`: Average |Ï†(Ï‰)| across frequencies
- `cf/phase_std`: Phase variation (higher = more distributional structure)

### Validation Metrics
- `debug/q_manual_mean`: Q-value via finite difference Ï†'(0)
- `debug/rewards_mean/std/min/max`: Reward statistics from sampled batch
- `debug/reward_cf_phase_std`: Phase variation in reward CF

### Performance Metrics
- `charts/episodic_return`: Episode total reward (target: 200-500 for CartPole)
- `charts/episodic_length`: Episode length
- `charts/SPS`: Training speed (steps per second)

---

## Key Differences from Standard DQN

| Aspect | Standard DQN | CF-DQN |
|--------|-------------|---------|
| **Output** | Scalar Q(s,a) | Complex CF Ï†(s,a,Ï‰) âˆˆ â„‚^K |
| **Target** | r + Î³ max_a' Q(s', a') | Ï†_r(Ï‰) Â· Ï†_future(Î³Ï‰) |
| **Loss** | Huber/MSE on scalars | L2 on complex vectors + Ï†(0) penalty |
| **Action Selection** | Direct argmax Q | Collapse CF â†’ Q, then argmax |
| **Information** | Point estimate | Full return distribution |

---

## Key Differences from C51

| Aspect | C51 | CF-DQN |
|--------|-----|---------|
| **Domain** | Value space (bins) | Frequency space (Ï‰) |
| **Representation** | Discrete probabilities p_i | Continuous CF Ï†(Ï‰) |
| **Support** | Fixed [V_min, V_max] | Implicit (encoded in CF) |
| **Projection** | Categorical projection | CF interpolation |
| **Mean** | âˆ‘ p_i Â· z_i | Collapse via Ï†'(0) |
| **Constraints** | âˆ‘ p_i = 1, p_i â‰¥ 0 | \|Ï†(0)\| = 1, \|Ï†(Ï‰)\| â‰¤ 1 |

---

## Theoretical Foundations

### Characteristic Function Properties

1. **Definition**: Ï†(Ï‰) = E[exp(iÏ‰G)] where G is the return
2. **Zeroth Moment**: Ï†(0) = E[exp(0)] = 1
3. **First Derivative**: Ï†'(0) = iE[G] (mean return)
4. **Boundedness**: |Ï†(Ï‰)| â‰¤ 1 for all Ï‰
5. **Conjugate Symmetry**: If G is real, Ï†(-Ï‰) = Ï†Ì„(Ï‰)

### CF Bellman Operator

For return G = r + Î³G':
```
Ï†_G(Ï‰) = E[exp(iÏ‰(r + Î³G'))]
        = E[exp(iÏ‰r) Â· exp(iÏ‰Î³G')]
        = E[exp(iÏ‰r)] Â· E[exp(iÏ‰Î³G')]    [if r âŠ¥ G' | s,a]
        = Ï†_r(Ï‰) Â· Ï†_G'(Î³Ï‰)
```

**Frequency Scaling**: Ï†_{Î³G'}(Ï‰) = E[exp(iÏ‰Î³G')] = Ï†_G'(Î³Ï‰)

This is the distributional Bellman equation in the frequency domain.

---

## Current Implementation Status

### âœ… Implemented
- Network architecture with complex output
- Three-density frequency grid
- Polar CF interpolation
- Gaussian collapse for mean extraction
- Soft Ï†(0)=1 constraint
- CF Bellman operator with terminal state handling
- Double DQN style action selection
- Comprehensive diagnostic logging

### âš ï¸ Known Issues
1. **Magnitude constraint violations**: max|Ï†(Ï‰)| occasionally exceeds 1.0 (should be â‰¤ 1.0)
2. **Normalization not perfect**: |Ï†(0)| may deviate from 1.0 despite penalty
3. **Performance unknown**: No confirmed successful CartPole training (target: 200-500 episodic return)
4. **Theoretical gap**: Tabular CVI uses full transition distribution P(s', r | s, a); deep version samples single transitions

### ðŸ”§ Potential Improvements
1. **Magnitude-only normalization**: Scale by |Ï†(0)| without affecting phase
2. **LS collapse method**: Use least-squares quadratic fit instead of phase slope
3. **Tighter frequency range**: Reduce freq_max to 2.5-3.0 for better resolution
4. **Architectural constraints**: Force |Ï†(Ï‰)| â‰¤ 1 via sigmoid on magnitude
5. **Empirical validation**: Test on synthetic tabular problems with known ground truth

---

## Training Example

**Command**:
```bash
python cleanrl/cf_dqn.py --env_id CartPole-v1 --total_timesteps 500000 --track
```

**Expected Behavior** (if working correctly):
- Episodic return should increase from ~20 to 200-500 over training
- `cf/magnitude_at_zero` should converge to â‰ˆ 1.0
- `cf/max_magnitude` should stay â‰¤ 1.05
- `losses/q_values` should be positive and increase
- Phase std should remain non-zero (> 0.5) indicating distributional learning

**Failure Modes**:
- **Mode collapse**: Phase std â†’ 0, Q â†’ 0, agent fails to learn
- **Magnitude explosion**: max|Ï†(Ï‰)| >> 1, invalid CF
- **Poor collapse**: Q-values negative or diverging from finite difference method
